{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer Encoder for Human Activity Recognition - Google Colab Ready\n",
        "\n",
        "This notebook implements a custom Transformer Encoder-based model specifically designed for time-series sensor data classification in elderly care applications.\n",
        "\n",
        "## Transformer Architecture for HAR:\n",
        "- **Multi-Head Self-Attention**: Focuses on different parts of sensor sequences simultaneously\n",
        "- **Stacked Encoder Blocks**: 2 consecutive Transformer encoder layers with self-attention and feed-forward networks\n",
        "- **Positional Encoding**: Captures temporal relationships in sensor data\n",
        "- **Residual Connections**: Help with gradient flow and training stability\n",
        "- **Layer Normalization**: Stabilizes training and improves convergence\n",
        "- **Lightweight Design**: Optimized for embedded/real-time applications\n",
        "\n",
        "## Key Features:\n",
        "- **Sensor Data**: Accelerometer (x,y,z), Gyroscope (x,y,z), Heart Rate\n",
        "- **Actions**: 8 different human activities\n",
        "- **Input Shape**: 128 timesteps × 7 features (optimized for sensor windows)\n",
        "- **Output**: Dense classification layers for activity class probabilities\n",
        "- **Regularization**: Dropout and layer normalization for overfitting prevention\n",
        "- **Colab Ready**: Optimized for Google Colab execution with file upload support\n",
        "\n",
        "## Instructions for Google Colab:\n",
        "1. Upload your HAR_synthetic_full.csv file using the file upload cell below\n",
        "2. Run all cells sequentially\n",
        "3. Results will be saved and can be downloaded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q tensorflow scikit-learn pandas numpy matplotlib seaborn joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.metrics import TopKCategoricalAccuracy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import json\n",
        "import time\n",
        "import joblib\n",
        "from google.colab import files\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# File Upload for Google Colab\n",
        "print(\"Please upload your HAR_synthetic_full.csv file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded file name\n",
        "csv_filename = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded file: {csv_filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set mixed precision for better performance\n",
        "from tensorflow.keras import mixed_precision\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)\n",
        "\n",
        "# Memory optimization\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Loading and Preprocessing Functions\n",
        "\n",
        "def load_sensor_data(csv_path):\n",
        "    \"\"\"Load and preprocess sensor data from CSV\"\"\"\n",
        "    print(\"Loading sensor data...\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    \n",
        "    # Remove rows with NaN labels\n",
        "    df = df.dropna(subset=['label'])\n",
        "    \n",
        "    print(f\"Data shape: {df.shape}\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "    print(f\"Unique labels: {df['label'].unique()}\")\n",
        "    print(f\"Label distribution:\")\n",
        "    print(df['label'].value_counts())\n",
        "    \n",
        "    return df\n",
        "\n",
        "def create_sequences(df, sequence_length=128, overlap=0.5):\n",
        "    \"\"\"Create sequences from sensor data for Transformer input\"\"\"\n",
        "    print(f\"Creating sequences with length {sequence_length} and overlap {overlap}...\")\n",
        "    \n",
        "    # Feature columns (excluding timestamp and label)\n",
        "    feature_cols = ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z', 'heart_rate_bpm']\n",
        "    \n",
        "    sequences = []\n",
        "    labels = []\n",
        "    \n",
        "    # Group by label to create sequences for each activity\n",
        "    for label in df['label'].unique():\n",
        "        label_data = df[df['label'] == label].copy()\n",
        "        \n",
        "        # Sort by timestamp to maintain temporal order\n",
        "        label_data = label_data.sort_values('timestamp_ms')\n",
        "        \n",
        "        # Extract features\n",
        "        features = label_data[feature_cols].values\n",
        "        \n",
        "        # Create overlapping sequences\n",
        "        step_size = int(sequence_length * (1 - overlap))\n",
        "        \n",
        "        for i in range(0, len(features) - sequence_length + 1, step_size):\n",
        "            sequence = features[i:i + sequence_length]\n",
        "            if len(sequence) == sequence_length:\n",
        "                sequences.append(sequence)\n",
        "                labels.append(label)\n",
        "    \n",
        "    sequences = np.array(sequences, dtype=np.float32)\n",
        "    labels = np.array(labels)\n",
        "    \n",
        "    print(f\"Created {len(sequences)} sequences\")\n",
        "    print(f\"Sequence shape: {sequences.shape}\")\n",
        "    \n",
        "    return sequences, labels\n",
        "\n",
        "def preprocess_data(sequences, labels):\n",
        "    \"\"\"Preprocess sequences and labels\"\"\"\n",
        "    print(\"Preprocessing data...\")\n",
        "    \n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    original_shape = sequences.shape\n",
        "    sequences_reshaped = sequences.reshape(-1, sequences.shape[-1])\n",
        "    sequences_normalized = scaler.fit_transform(sequences_reshaped)\n",
        "    sequences = sequences_normalized.reshape(original_shape)\n",
        "    \n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    labels_encoded = label_encoder.fit_transform(labels)\n",
        "    labels_onehot = to_categorical(labels_encoded)\n",
        "    \n",
        "    print(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
        "    print(f\"Classes: {label_encoder.classes_}\")\n",
        "    \n",
        "    return sequences, labels_onehot, label_encoder, scaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and preprocess the data\n",
        "# Use uploaded file for Colab\n",
        "csv_path = csv_filename\n",
        "\n",
        "# Load data\n",
        "df = load_sensor_data(csv_path)\n",
        "\n",
        "# Create sequences (128 timesteps for Transformer)\n",
        "sequences, labels = create_sequences(df, sequence_length=128, overlap=0.5)\n",
        "\n",
        "# Preprocess data\n",
        "X, y, label_encoder, scaler = preprocess_data(sequences, labels)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=np.argmax(y, axis=1)\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "print(f\"Number of features: {X_train.shape[2]}\")\n",
        "print(f\"Number of classes: {y_train.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Augmentation for Sensor Data\n",
        "\n",
        "def augment_sensor_sequence(sequence, noise_factor=0.1, time_shift=0.1):\n",
        "    \"\"\"Apply data augmentation to sensor sequences\"\"\"\n",
        "    augmented = sequence.copy()\n",
        "    \n",
        "    # Add Gaussian noise\n",
        "    if np.random.random() < 0.5:\n",
        "        noise = np.random.normal(0, noise_factor, sequence.shape)\n",
        "        augmented = augmented + noise\n",
        "    \n",
        "    # Time shifting (circular shift)\n",
        "    if np.random.random() < 0.3:\n",
        "        shift = int(sequence.shape[0] * time_shift * np.random.uniform(-1, 1))\n",
        "        augmented = np.roll(augmented, shift, axis=0)\n",
        "    \n",
        "    # Scaling\n",
        "    if np.random.random() < 0.3:\n",
        "        scale_factor = np.random.uniform(0.9, 1.1)\n",
        "        augmented = augmented * scale_factor\n",
        "    \n",
        "    return augmented\n",
        "\n",
        "def create_augmented_data(X_train, y_train, augmentation_factor=2):\n",
        "    \"\"\"Create augmented training data\"\"\"\n",
        "    print(f\"Creating augmented data with factor {augmentation_factor}...\")\n",
        "    \n",
        "    X_augmented = []\n",
        "    y_augmented = []\n",
        "    \n",
        "    for i in range(len(X_train)):\n",
        "        # Original data\n",
        "        X_augmented.append(X_train[i])\n",
        "        y_augmented.append(y_train[i])\n",
        "        \n",
        "        # Augmented data\n",
        "        for _ in range(augmentation_factor):\n",
        "            aug_seq = augment_sensor_sequence(X_train[i])\n",
        "            X_augmented.append(aug_seq)\n",
        "            y_augmented.append(y_train[i])\n",
        "    \n",
        "    return np.array(X_augmented), np.array(y_augmented)\n",
        "\n",
        "# Create augmented training data\n",
        "X_train_aug, y_train_aug = create_augmented_data(X_train, y_train, augmentation_factor=1)\n",
        "\n",
        "print(f\"Original training data: {X_train.shape}\")\n",
        "print(f\"Augmented training data: {X_train_aug.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformer Encoder Implementation for HAR\n",
        "\n",
        "class PositionalEncoding(Layer):\n",
        "    \"\"\"Positional encoding for time-series data\"\"\"\n",
        "    \n",
        "    def __init__(self, max_len=128, d_model=64, **kwargs):\n",
        "        super(PositionalEncoding, self).__init__(**kwargs)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # Create positional encoding matrix\n",
        "        pos_encoding = np.zeros((max_len, d_model))\n",
        "        position = np.arange(0, max_len, dtype=np.float32)[:, np.newaxis]\n",
        "        \n",
        "        div_term = np.exp(np.arange(0, d_model, 2, dtype=np.float32) * \n",
        "                         -(np.log(10000.0) / d_model))\n",
        "        \n",
        "        pos_encoding[:, 0::2] = np.sin(position * div_term)\n",
        "        pos_encoding[:, 1::2] = np.cos(position * div_term)\n",
        "        \n",
        "        # Store as numpy array, will be converted to match input dtype in call()\n",
        "        self.pos_encoding_np = pos_encoding[np.newaxis, :, :]\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "        # Convert positional encoding to match input dtype\n",
        "        pos_encoding = tf.cast(self.pos_encoding_np, dtype=inputs.dtype)\n",
        "        return inputs + pos_encoding[:, :seq_len, :]\n",
        "    \n",
        "    def get_config(self):\n",
        "        config = super(PositionalEncoding, self).get_config()\n",
        "        config.update({\n",
        "            'max_len': self.max_len,\n",
        "            'd_model': self.d_model\n",
        "        })\n",
        "        return config\n",
        "\n",
        "class TransformerEncoderBlock(Layer):\n",
        "    \"\"\"Single Transformer Encoder Block\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model=64, num_heads=8, dff=128, dropout_rate=0.1, **kwargs):\n",
        "        super(TransformerEncoderBlock, self).__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff\n",
        "        self.dropout_rate = dropout_rate\n",
        "        \n",
        "        # Multi-head self-attention\n",
        "        self.mha = MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=d_model,\n",
        "            dropout=dropout_rate\n",
        "        )\n",
        "        \n",
        "        # Feed-forward network\n",
        "        self.ffn = Sequential([\n",
        "            Dense(dff, activation='relu'),\n",
        "            Dense(d_model)\n",
        "        ])\n",
        "        \n",
        "        # Layer normalization\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Dropout\n",
        "        self.dropout1 = Dropout(dropout_rate)\n",
        "        self.dropout2 = Dropout(dropout_rate)\n",
        "    \n",
        "    def call(self, inputs, training=None):\n",
        "        # Multi-head self-attention\n",
        "        attn_output = self.mha(inputs, inputs, training=training)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        \n",
        "        # Feed-forward network\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "        \n",
        "        return out2\n",
        "    \n",
        "    def get_config(self):\n",
        "        config = super(TransformerEncoderBlock, self).get_config()\n",
        "        config.update({\n",
        "            'd_model': self.d_model,\n",
        "            'num_heads': self.num_heads,\n",
        "            'dff': self.dff,\n",
        "            'dropout_rate': self.dropout_rate\n",
        "        })\n",
        "        return config\n",
        "\n",
        "def create_transformer_har_model(input_shape, num_classes, d_model=64, num_heads=8, num_layers=2, dff=128, dropout_rate=0.1):\n",
        "    \"\"\"Create Transformer Encoder model for HAR\"\"\"\n",
        "    \n",
        "    inputs = Input(shape=input_shape)\n",
        "    \n",
        "    # Input projection to d_model dimensions\n",
        "    x = Dense(d_model)(inputs)\n",
        "    \n",
        "    # Add positional encoding\n",
        "    x = PositionalEncoding(max_len=input_shape[0], d_model=d_model)(x)\n",
        "    \n",
        "    # Stack of Transformer encoder blocks\n",
        "    for _ in range(num_layers):\n",
        "        x = TransformerEncoderBlock(\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dff=dff,\n",
        "            dropout_rate=dropout_rate\n",
        "        )(x)\n",
        "    \n",
        "    # Global average pooling\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    \n",
        "    # Classification head\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    outputs = Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def create_lightweight_transformer(input_shape, num_classes):\n",
        "    \"\"\"Create lightweight Transformer for faster training\"\"\"\n",
        "    return create_transformer_har_model(\n",
        "        input_shape=input_shape,\n",
        "        num_classes=num_classes,\n",
        "        d_model=32,\n",
        "        num_heads=4,\n",
        "        num_layers=2,\n",
        "        dff=64,\n",
        "        dropout_rate=0.1\n",
        "    )\n",
        "\n",
        "def create_standard_transformer(input_shape, num_classes):\n",
        "    \"\"\"Create standard Transformer model\"\"\"\n",
        "    return create_transformer_har_model(\n",
        "        input_shape=input_shape,\n",
        "        num_classes=num_classes,\n",
        "        d_model=64,\n",
        "        num_heads=8,\n",
        "        num_layers=2,\n",
        "        dff=128,\n",
        "        dropout_rate=0.1\n",
        "    )\n",
        "\n",
        "def create_deep_transformer(input_shape, num_classes):\n",
        "    \"\"\"Create deep Transformer with more layers\"\"\"\n",
        "    return create_transformer_har_model(\n",
        "        input_shape=input_shape,\n",
        "        num_classes=num_classes,\n",
        "        d_model=64,\n",
        "        num_heads=8,\n",
        "        num_layers=3,\n",
        "        dff=128,\n",
        "        dropout_rate=0.1\n",
        "    )\n",
        "\n",
        "def create_wide_transformer(input_shape, num_classes):\n",
        "    \"\"\"Create wide Transformer with more attention heads\"\"\"\n",
        "    return create_transformer_har_model(\n",
        "        input_shape=input_shape,\n",
        "        num_classes=num_classes,\n",
        "        d_model=128,\n",
        "        num_heads=16,\n",
        "        num_layers=2,\n",
        "        dff=256,\n",
        "        dropout_rate=0.1\n",
        "    )\n",
        "\n",
        "# Model selection\n",
        "TRANSFORMER_MODEL_CHOICES = {\n",
        "    1: (\"Lightweight Transformer\", create_lightweight_transformer),\n",
        "    2: (\"Standard Transformer\", create_standard_transformer),\n",
        "    3: (\"Deep Transformer\", create_deep_transformer),\n",
        "    4: (\"Wide Transformer\", create_wide_transformer)\n",
        "}\n",
        "\n",
        "print(\"Available Transformer models:\")\n",
        "for key, (name, _) in TRANSFORMER_MODEL_CHOICES.items():\n",
        "    print(f\"{key}. {name}\")\n",
        "\n",
        "# Get input shape and number of classes\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "print(f\"\\nInput shape: {input_shape}\")\n",
        "print(f\"Number of classes: {num_classes}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build and compile the Transformer model\n",
        "model_name, model_func = TRANSFORMER_MODEL_CHOICES[2]  # Using Standard Transformer\n",
        "print(f\"Building {model_name}...\")\n",
        "\n",
        "model = model_func(input_shape, num_classes)\n",
        "\n",
        "# Custom loss function with label smoothing\n",
        "def smooth_categorical_crossentropy(y_true, y_pred, alpha=0.1):\n",
        "    \"\"\"Label smoothing for better generalization\"\"\"\n",
        "    num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n",
        "    y_true_smooth = y_true * (1.0 - alpha) + alpha / num_classes\n",
        "    return tf.keras.losses.categorical_crossentropy(y_true_smooth, y_pred)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(\n",
        "    learning_rate=0.001,\n",
        "    weight_decay=1e-4,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999\n",
        ")\n",
        "\n",
        "# Metrics\n",
        "top_3_accuracy = TopKCategoricalAccuracy(k=3, name='top_3_accuracy')\n",
        "top_5_accuracy = TopKCategoricalAccuracy(k=5, name='top_5_accuracy')\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=smooth_categorical_crossentropy,\n",
        "    metrics=['accuracy', top_3_accuracy, top_5_accuracy]\n",
        ")\n",
        "\n",
        "total_params = model.count_params()\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Configuration\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=20,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1,\n",
        "        min_delta=0.001\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=10,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1,\n",
        "        cooldown=3\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        'best_transformer_har_model.keras',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1,\n",
        "        save_weights_only=False\n",
        "    )\n",
        "]\n",
        "\n",
        "# Data generator for training\n",
        "class SensorDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, X, y, batch_size=32, shuffle=True, augment=True):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment\n",
        "        self.indices = np.arange(len(X))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.X) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        X_batch = self.X[indices].copy()\n",
        "        y_batch = self.y[indices]\n",
        "\n",
        "        if self.augment:\n",
        "            for i in range(len(X_batch)):\n",
        "                if np.random.random() < 0.3:  # 30% chance of augmentation\n",
        "                    X_batch[i] = augment_sensor_sequence(X_batch[i])\n",
        "\n",
        "        return X_batch, y_batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "# Create data generators\n",
        "train_gen = SensorDataGenerator(X_train_aug, y_train_aug, batch_size=32, augment=True)\n",
        "val_gen = SensorDataGenerator(X_test, y_test, batch_size=32, augment=False, shuffle=False)\n",
        "\n",
        "print(f\"Training batches: {len(train_gen)}\")\n",
        "print(f\"Validation batches: {len(val_gen)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the Transformer model\n",
        "print(f\"Starting training with {model_name}...\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=50,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_results = model.evaluate(val_gen, verbose=0)\n",
        "test_loss = test_results[0]\n",
        "test_acc = test_results[1]\n",
        "test_top3 = test_results[2] if len(test_results) > 2 else 0\n",
        "test_top5 = test_results[3] if len(test_results) > 3 else 0\n",
        "\n",
        "print(f\"\\n=== TRANSFORMER RESULTS ===\")\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Parameters: {total_params:,}\")\n",
        "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
        "print(f\"Top-3 Accuracy: {test_top3*100:.2f}%\")\n",
        "print(f\"Top-5 Accuracy: {test_top5*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization and Analysis\n",
        "\n",
        "# Training history plots\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
        "plt.title(f'{model_name} - Accuracy\\nFinal: {test_acc*100:.1f}%', fontweight='bold')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Loss\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
        "plt.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
        "plt.title('Training Loss', fontweight='bold')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Top-3 Accuracy\n",
        "plt.subplot(2, 3, 3)\n",
        "if 'top_3_accuracy' in history.history:\n",
        "    plt.plot(history.history['top_3_accuracy'], label='Train Top-3', linewidth=2)\n",
        "    plt.plot(history.history['val_top_3_accuracy'], label='Val Top-3', linewidth=2)\n",
        "    plt.title(f'Top-3 Accuracy\\nFinal: {test_top3*100:.1f}%', fontweight='bold')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Top-3 Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Learning rate\n",
        "plt.subplot(2, 3, 4)\n",
        "if 'lr' in history.history:\n",
        "    plt.plot(history.history['lr'], linewidth=2, color='red')\n",
        "    plt.title('Learning Rate Schedule', fontweight='bold')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.yscale('log')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Model summary\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.text(0.1, 0.8, f'Model: {model_name}', fontsize=12, fontweight='bold')\n",
        "plt.text(0.1, 0.7, f'Parameters: {total_params:,}', fontsize=10)\n",
        "plt.text(0.1, 0.6, f'Final Accuracy: {test_acc*100:.2f}%', fontsize=10)\n",
        "plt.text(0.1, 0.5, f'Top-3 Accuracy: {test_top3*100:.2f}%', fontsize=10)\n",
        "plt.text(0.1, 0.4, f'Input Shape: {input_shape}', fontsize=9)\n",
        "plt.text(0.1, 0.3, f'Classes: {num_classes}', fontsize=9)\n",
        "plt.xlim(0, 1)\n",
        "plt.ylim(0, 1)\n",
        "plt.axis('off')\n",
        "plt.title('Model Summary', fontweight='bold')\n",
        "\n",
        "# Confusion matrix\n",
        "plt.subplot(2, 3, 6)\n",
        "y_pred = model.predict(X_test, verbose=0)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=label_encoder.classes_, \n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.title('Confusion Matrix', fontweight='bold')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed Classification Report\n",
        "print(\"\\n=== TRANSFORMER CLASSIFICATION REPORT ===\")\n",
        "print(classification_report(y_true_classes, y_pred_classes, \n",
        "                          target_names=label_encoder.classes_))\n",
        "\n",
        "# Per-class metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "precision = precision_score(y_true_classes, y_pred_classes, average=None)\n",
        "recall = recall_score(y_true_classes, y_pred_classes, average=None)\n",
        "f1 = f1_score(y_true_classes, y_pred_classes, average=None)\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Class': label_encoder.classes_,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1-Score': f1\n",
        "})\n",
        "\n",
        "print(\"\\n=== PER-CLASS METRICS ===\")\n",
        "print(metrics_df.round(3))\n",
        "\n",
        "# Save metrics\n",
        "metrics_df.to_csv('transformer_har_metrics.csv', index=False)\n",
        "print(\"\\nTransformer metrics saved to 'transformer_har_metrics.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Testing and Prediction\n",
        "\n",
        "# Test on a few samples\n",
        "print(\"\\n=== TRANSFORMER SAMPLE PREDICTIONS ===\")\n",
        "for i in range(5):\n",
        "    sample_idx = np.random.randint(0, len(X_test))\n",
        "    sample = X_test[sample_idx:sample_idx+1]\n",
        "    \n",
        "    prediction = model.predict(sample, verbose=0)\n",
        "    predicted_class = np.argmax(prediction)\n",
        "    confidence = np.max(prediction)\n",
        "    actual_class = np.argmax(y_test[sample_idx])\n",
        "    \n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"  Predicted: {label_encoder.classes_[predicted_class]} ({confidence*100:.1f}%)\")\n",
        "    print(f\"  Actual: {label_encoder.classes_[actual_class]}\")\n",
        "    print(f\"  Correct: {'✓' if predicted_class == actual_class else '✗'}\")\n",
        "    print()\n",
        "\n",
        "# Save the model\n",
        "model.save('transformer_har_model_final.keras')\n",
        "print(\"\\nTransformer model saved as 'transformer_har_model_final.keras'\")\n",
        "\n",
        "# Save preprocessing objects\n",
        "import joblib\n",
        "joblib.dump(scaler, 'transformer_sensor_scaler.pkl')\n",
        "joblib.dump(label_encoder, 'transformer_sensor_label_encoder.pkl')\n",
        "print(\"Transformer preprocessing objects saved\")\n",
        "\n",
        "print(\"\\n=== TRANSFORMER TRAINING COMPLETE ===\")\n",
        "print(f\"Final Test Accuracy: {test_acc*100:.2f}%\")\n",
        "print(f\"Model Parameters: {total_params:,}\")\n",
        "print(f\"Classes: {', '.join(label_encoder.classes_)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive Results Collection for Journal Publication\n",
        "\n",
        "print(\"=== COLLECTING COMPREHENSIVE RESULTS ===\")\n",
        "\n",
        "# Get predictions for comprehensive evaluation\n",
        "y_pred_proba = model.predict(X_test, verbose=0)\n",
        "y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate comprehensive metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "accuracy = np.mean(y_pred == y_true)\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "# ROC AUC (multi-class)\n",
        "try:\n",
        "    auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')\n",
        "except:\n",
        "    auc = 0.0\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Classification report\n",
        "class_report = classification_report(y_true, y_pred, target_names=label_encoder.classes_, output_dict=True)\n",
        "\n",
        "# Store comprehensive results\n",
        "results = {\n",
        "    'model_name': 'Transformer Encoder',\n",
        "    'accuracy': float(accuracy),\n",
        "    'precision': float(precision),\n",
        "    'recall': float(recall),\n",
        "    'f1_score': float(f1),\n",
        "    'auc': float(auc),\n",
        "    'confusion_matrix': cm.tolist(),\n",
        "    'classification_report': class_report,\n",
        "    'predictions': y_pred.tolist(),\n",
        "    'true_labels': y_true.tolist(),\n",
        "    'prediction_probabilities': y_pred_proba.tolist(),\n",
        "    'training_time': training_time,\n",
        "    'total_parameters': int(total_params),\n",
        "    'sequence_length': 128,\n",
        "    'classes': label_encoder.classes_.tolist()\n",
        "}\n",
        "\n",
        "# Save results as JSON\n",
        "with open('transformer_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "# Save detailed classification report\n",
        "class_report_df = pd.DataFrame(class_report).T\n",
        "class_report_df.to_csv('transformer_classification_report.csv')\n",
        "\n",
        "# Save the model\n",
        "model.save('transformer_model_final.keras')\n",
        "\n",
        "# Save preprocessing objects\n",
        "joblib.dump(scaler, 'transformer_sensor_scaler.pkl')\n",
        "joblib.dump(label_encoder, 'transformer_sensor_label_encoder.pkl')\n",
        "\n",
        "print(\"=== RESULTS SUMMARY ===\")\n",
        "print(f\"Model: Transformer Encoder\")\n",
        "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
        "print(f\"Precision: {precision*100:.2f}%\")\n",
        "print(f\"Recall: {recall*100:.2f}%\")\n",
        "print(f\"F1-Score: {f1*100:.2f}%\")\n",
        "print(f\"AUC: {auc*100:.2f}%\")\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"Parameters: {total_params:,}\")\n",
        "\n",
        "print(\"\\n=== FILES SAVED ===\")\n",
        "print(\"- transformer_model_final.keras\")\n",
        "print(\"- transformer_results.json\")\n",
        "print(\"- transformer_classification_report.csv\")\n",
        "print(\"- transformer_sensor_scaler.pkl\")\n",
        "print(\"- transformer_sensor_label_encoder.pkl\")\n",
        "print(\"- transformer_results.png (from visualization cell)\")\n",
        "\n",
        "print(\"\\n=== DOWNLOAD FILES ===\")\n",
        "print(\"Run the next cell to download all results files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download Results Files for Google Colab\n",
        "print(\"Downloading all result files...\")\n",
        "\n",
        "# Create a zip file with all results\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# List of files to include in the download\n",
        "files_to_download = [\n",
        "    'transformer_model_final.keras',\n",
        "    'transformer_results.json', \n",
        "    'transformer_classification_report.csv',\n",
        "    'transformer_sensor_scaler.pkl',\n",
        "    'transformer_sensor_label_encoder.pkl'\n",
        "]\n",
        "\n",
        "# Add PNG file if it exists\n",
        "if os.path.exists('transformer_results.png'):\n",
        "    files_to_download.append('transformer_results.png')\n",
        "\n",
        "# Create zip file\n",
        "with zipfile.ZipFile('transformer_results.zip', 'w') as zipf:\n",
        "    for file in files_to_download:\n",
        "        if os.path.exists(file):\n",
        "            zipf.write(file)\n",
        "            print(f\"Added {file} to download\")\n",
        "\n",
        "# Download the zip file\n",
        "files.download('transformer_results.zip')\n",
        "\n",
        "print(\"\\n=== TRANSFORMER MODEL EXECUTION COMPLETE ===\")\n",
        "print(\"All results have been saved and downloaded!\")\n",
        "print(\"You can now use these files for your journal publication.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
