{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ResNet-Transformer for Human Activity Recognition (Sensor Data) - Google Colab Ready\n",
        "\n",
        "This notebook implements a hybrid 1D ResNet + Transformer encoder architecture tailored for time-series sensor data classification on the same dataset and pipeline as your other notebooks.\n",
        "\n",
        "## Instructions for Google Colab:\n",
        "1. Upload your HAR_synthetic_full.csv file using the file upload cell below\n",
        "2. Run all cells sequentially\n",
        "3. Results will be saved and can be downloaded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install -q tensorflow scikit-learn pandas numpy matplotlib seaborn joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.metrics import TopKCategoricalAccuracy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import json\n",
        "import time\n",
        "import joblib\n",
        "from google.colab import files\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# File Upload for Google Colab\n",
        "print(\"Please upload your HAR_synthetic_full.csv file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded file name\n",
        "csv_filename = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded file: {csv_filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mixed precision and memory growth\n",
        "from tensorflow.keras import mixed_precision\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data loading and preprocessing (reuse same pipeline)\n",
        "\n",
        "def load_sensor_data(csv_path):\n",
        "    print(\"Loading sensor data...\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df.dropna(subset=['label'])\n",
        "    print(f\"Data shape: {df.shape}\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "    print(f\"Unique labels: {df['label'].unique()}\")\n",
        "    print(\"Label distribution:\")\n",
        "    print(df['label'].value_counts())\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_sequences(df, sequence_length=128, overlap=0.5):\n",
        "    print(f\"Creating sequences with length {sequence_length} and overlap {overlap}...\")\n",
        "    feature_cols = ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z', 'heart_rate_bpm']\n",
        "    sequences, labels = [], []\n",
        "    step_size = int(sequence_length * (1 - overlap))\n",
        "\n",
        "    for label in df['label'].unique():\n",
        "        label_data = df[df['label'] == label].copy()\n",
        "        label_data = label_data.sort_values('timestamp_ms')\n",
        "        features = label_data[feature_cols].values\n",
        "        for i in range(0, len(features) - sequence_length + 1, step_size):\n",
        "            seq = features[i:i + sequence_length]\n",
        "            if len(seq) == sequence_length:\n",
        "                sequences.append(seq)\n",
        "                labels.append(label)\n",
        "\n",
        "    sequences = np.array(sequences, dtype=np.float32)\n",
        "    labels = np.array(labels)\n",
        "    print(f\"Created {len(sequences)} sequences with shape {sequences.shape}\")\n",
        "    return sequences, labels\n",
        "\n",
        "\n",
        "def preprocess_data(sequences, labels):\n",
        "    print(\"Preprocessing data...\")\n",
        "    scaler = StandardScaler()\n",
        "    original_shape = sequences.shape\n",
        "    flat = sequences.reshape(-1, sequences.shape[-1])\n",
        "    norm = scaler.fit_transform(flat)\n",
        "    sequences = norm.reshape(original_shape)\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    labels_idx = le.fit_transform(labels)\n",
        "    labels_oh = to_categorical(labels_idx)\n",
        "\n",
        "    print(f\"Number of classes: {len(le.classes_)}\")\n",
        "    print(f\"Classes: {le.classes_}\")\n",
        "    return sequences, labels_oh, le, scaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset and split\n",
        "# Use uploaded file for Colab\n",
        "csv_path = csv_filename\n",
        "\n",
        "df = load_sensor_data(csv_path)\n",
        "sequences, labels = create_sequences(df, sequence_length=128, overlap=0.5)\n",
        "X, y, label_encoder, scaler = preprocess_data(sequences, labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=np.argmax(y, axis=1)\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "print(f\"Number of features: {X_train.shape[2]}\")\n",
        "print(f\"Number of classes: {y_train.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data augmentation (same as Transformer)\n",
        "\n",
        "def augment_sensor_sequence(sequence, noise_factor=0.1, time_shift=0.1):\n",
        "    augmented = sequence.copy()\n",
        "    if np.random.random() < 0.5:\n",
        "        noise = np.random.normal(0, noise_factor, sequence.shape)\n",
        "        augmented = augmented + noise\n",
        "    if np.random.random() < 0.3:\n",
        "        shift = int(sequence.shape[0] * time_shift * np.random.uniform(-1, 1))\n",
        "        augmented = np.roll(augmented, shift, axis=0)\n",
        "    if np.random.random() < 0.3:\n",
        "        scale_factor = np.random.uniform(0.9, 1.1)\n",
        "        augmented = augmented * scale_factor\n",
        "    return augmented\n",
        "\n",
        "\n",
        "def create_augmented_data(X_train, y_train, augmentation_factor=1):\n",
        "    print(f\"Creating augmented data with factor {augmentation_factor}...\")\n",
        "    X_aug, y_aug = [], []\n",
        "    for i in range(len(X_train)):\n",
        "        X_aug.append(X_train[i]); y_aug.append(y_train[i])\n",
        "        for _ in range(augmentation_factor):\n",
        "            X_aug.append(augment_sensor_sequence(X_train[i]))\n",
        "            y_aug.append(y_train[i])\n",
        "    return np.array(X_aug), np.array(y_aug)\n",
        "\n",
        "X_train_aug, y_train_aug = create_augmented_data(X_train, y_train, augmentation_factor=1)\n",
        "print(f\"Original training data: {X_train.shape}\")\n",
        "print(f\"Augmented training data: {X_train_aug.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ResNet-Transformer model\n",
        "\n",
        "class ResidualBlock1D(Layer):\n",
        "    def __init__(self, filters, kernel_size=3, stride=1, use_projection=False, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.use_projection = use_projection\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.conv1 = Conv1D(filters, kernel_size, padding='same', strides=stride)\n",
        "        self.bn1 = LayerNormalization()\n",
        "        self.act1 = Activation('relu')\n",
        "        self.conv2 = Conv1D(filters, kernel_size, padding='same', strides=1)\n",
        "        self.bn2 = LayerNormalization()\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        self.proj = Conv1D(filters, 1, strides=stride, padding='same') if use_projection else None\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = self.act1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x, training=training)\n",
        "        x = self.dropout(x, training=training)\n",
        "        shortcut = self.proj(inputs) if self.proj is not None else inputs\n",
        "        x = Add()([x, shortcut])\n",
        "        return Activation('relu')(x)\n",
        "\n",
        "\n",
        "class PositionalEncoding(Layer):\n",
        "    def __init__(self, max_len, d_model, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        pos_encoding = np.zeros((max_len, d_model))\n",
        "        position = np.arange(0, max_len, dtype=np.float32)[:, np.newaxis]\n",
        "        div_term = np.exp(np.arange(0, d_model, 2, dtype=np.float32) * -(np.log(10000.0) / d_model))\n",
        "        pos_encoding[:, 0::2] = np.sin(position * div_term)\n",
        "        pos_encoding[:, 1::2] = np.cos(position * div_term)\n",
        "        self.pos_encoding_np = pos_encoding[np.newaxis, :, :]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "        pos_encoding = tf.cast(self.pos_encoding_np, dtype=inputs.dtype)\n",
        "        return inputs + pos_encoding[:, :seq_len, :]\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(Layer):\n",
        "    def __init__(self, d_model=64, num_heads=8, dff=128, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n",
        "        self.ffn = Sequential([Dense(dff, activation='relu'), Dense(d_model)])\n",
        "        self.ln1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.ln2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.do1 = Dropout(dropout_rate)\n",
        "        self.do2 = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        attn = self.mha(inputs, inputs, training=training)\n",
        "        attn = self.do1(attn, training=training)\n",
        "        out1 = self.ln1(inputs + attn)\n",
        "        ffn = self.ffn(out1)\n",
        "        ffn = self.do2(ffn, training=training)\n",
        "        out2 = self.ln2(out1 + ffn)\n",
        "        return out2\n",
        "\n",
        "\n",
        "def create_resnet_transformer(input_shape, num_classes,\n",
        "                               resnet_filters=[32, 64],\n",
        "                               res_blocks_per_stage=2,\n",
        "                               d_model=64, num_heads=8, num_layers=2, dff=128,\n",
        "                               dropout_rate=0.1):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Stem\n",
        "    x = Conv1D(resnet_filters[0], 7, strides=1, padding='same')(inputs)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # ResNet stages\n",
        "    for stage_idx, filters in enumerate(resnet_filters):\n",
        "        for block_idx in range(res_blocks_per_stage):\n",
        "            use_proj = (block_idx == 0 and stage_idx > 0)\n",
        "            stride = 2 if use_proj else 1\n",
        "            x = ResidualBlock1D(filters, kernel_size=3, stride=stride, use_projection=use_proj, dropout_rate=dropout_rate)(x)\n",
        "\n",
        "    # Project to transformer dim\n",
        "    x = Conv1D(d_model, 1, padding='same')(x)\n",
        "\n",
        "    # Positional encoding\n",
        "    x = PositionalEncoding(max_len=input_shape[0], d_model=d_model)(x)\n",
        "\n",
        "    # Transformer encoder stack\n",
        "    for _ in range(num_layers):\n",
        "        x = TransformerEncoderBlock(d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate)(x)\n",
        "\n",
        "    # Head\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    outputs = Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Shapes and model\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "num_classes = y_train.shape[1]\n",
        "model = create_resnet_transformer(input_shape, num_classes)\n",
        "\n",
        "# Summary\n",
        "total_params = model.count_params()\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile, callbacks, and data generators\n",
        "\n",
        "# Loss with label smoothing\n",
        "def smooth_categorical_crossentropy(y_true, y_pred, alpha=0.1):\n",
        "    num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n",
        "    y_true_smooth = y_true * (1.0 - alpha) + alpha / num_classes\n",
        "    return tf.keras.losses.categorical_crossentropy(y_true_smooth, y_pred)\n",
        "\n",
        "optimizer = AdamW(learning_rate=0.001, weight_decay=1e-4)\n",
        "\n",
        "# Metrics\n",
        "top_3_accuracy = TopKCategoricalAccuracy(k=3, name='top_3_accuracy')\n",
        "top_5_accuracy = TopKCategoricalAccuracy(k=5, name='top_5_accuracy')\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=smooth_categorical_crossentropy,\n",
        "              metrics=['accuracy', top_3_accuracy, top_5_accuracy])\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True, verbose=1, min_delta=0.001),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7, verbose=1, cooldown=3),\n",
        "    ModelCheckpoint('best_resnet_transformer_har_model.keras', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "]\n",
        "\n",
        "class SensorDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, X, y, batch_size=32, shuffle=True, augment=True):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment\n",
        "        self.indices = np.arange(len(X))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.X) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        X_batch = self.X[indices].copy()\n",
        "        y_batch = self.y[indices]\n",
        "        if self.augment:\n",
        "            for i in range(len(X_batch)):\n",
        "                if np.random.random() < 0.3:\n",
        "                    X_batch[i] = augment_sensor_sequence(X_batch[i])\n",
        "        return X_batch, y_batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "train_gen = SensorDataGenerator(X_train_aug, y_train_aug, batch_size=32, augment=True)\n",
        "val_gen = SensorDataGenerator(X_test, y_test, batch_size=32, augment=False, shuffle=False)\n",
        "\n",
        "print(f\"Training batches: {len(train_gen)}\")\n",
        "print(f\"Validation batches: {len(val_gen)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate\n",
        "print(\"Starting training ResNet-Transformer...\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=50,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "test_results = model.evaluate(val_gen, verbose=0)\n",
        "\n",
        "test_loss = test_results[0]\n",
        "test_acc = test_results[1]\n",
        "test_top3 = test_results[2] if len(test_results) > 2 else 0\n",
        "test_top5 = test_results[3] if len(test_results) > 3 else 0\n",
        "\n",
        "print(f\"\\n=== RESNET-TRANSFORMER RESULTS ===\")\n",
        "print(f\"Parameters: {total_params:,}\")\n",
        "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
        "print(f\"Top-3 Accuracy: {test_top3*100:.2f}%\")\n",
        "print(f\"Top-5 Accuracy: {test_top5*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "plt.title('Accuracy')\n",
        "plt.legend(); plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Loss')\n",
        "plt.legend(); plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 3, 3)\n",
        "if 'top_3_accuracy' in history.history:\n",
        "    plt.plot(history.history['top_3_accuracy'], label='Train Top-3')\n",
        "    plt.plot(history.history['val_top_3_accuracy'], label='Val Top-3')\n",
        "    plt.title('Top-3 Accuracy')\n",
        "    plt.legend(); plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.text(0.1, 0.8, 'Model: ResNet-Transformer', fontsize=12, fontweight='bold')\n",
        "plt.text(0.1, 0.7, f'Parameters: {total_params:,}', fontsize=10)\n",
        "plt.text(0.1, 0.6, f'Final Accuracy: {test_acc*100:.2f}%', fontsize=10)\n",
        "plt.text(0.1, 0.4, f'Input Shape: {input_shape}', fontsize=9)\n",
        "plt.text(0.1, 0.3, f'Classes: {num_classes}', fontsize=9)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(2, 3, 6)\n",
        "y_pred = model.predict(X_test, verbose=0)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted'); plt.ylabel('Actual')\n",
        "\n",
        "plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification report and saving artifacts\n",
        "print(\"\\n=== RESNET-TRANSFORMER CLASSIFICATION REPORT ===\")\n",
        "print(classification_report(y_true_classes, y_pred_classes, target_names=label_encoder.classes_))\n",
        "\n",
        "# Save model and preprocessors\n",
        "model.save('resnet_transformer_model_final.keras')\n",
        "joblib.dump(scaler, 'resnet_transformer_sensor_scaler.pkl')\n",
        "joblib.dump(label_encoder, 'resnet_transformer_sensor_label_encoder.pkl')\n",
        "print(\"Saved model and preprocessing objects\")\n",
        "\n",
        "# Save metrics\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Class': label_encoder.classes_,\n",
        "    'Precision': pd.Series(dtype=float),\n",
        "    'Recall': pd.Series(dtype=float),\n",
        "    'F1-Score': pd.Series(dtype=float)\n",
        "})\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "precision = precision_score(y_true_classes, y_pred_classes, average=None)\n",
        "recall = recall_score(y_true_classes, y_pred_classes, average=None)\n",
        "f1 = f1_score(y_true_classes, y_pred_classes, average=None)\n",
        "metrics_df['Precision'] = precision\n",
        "metrics_df['Recall'] = recall\n",
        "metrics_df['F1-Score'] = f1\n",
        "metrics_df.to_csv('resnet_transformer_classification_report.csv', index=False)\n",
        "print(\"Saved metrics to 'resnet_transformer_classification_report.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive Results Collection for Journal Publication\n",
        "\n",
        "print(\"=== COLLECTING COMPREHENSIVE RESULTS ===\")\n",
        "\n",
        "# Get predictions for comprehensive evaluation\n",
        "y_pred_proba = model.predict(X_test, verbose=0)\n",
        "y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate comprehensive metrics\n",
        "accuracy = np.mean(y_pred == y_true)\n",
        "precision_macro = precision_score(y_true, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_true, y_pred, average='macro')\n",
        "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "# ROC AUC (multi-class)\n",
        "try:\n",
        "    auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')\n",
        "except:\n",
        "    auc = 0.0\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Classification report\n",
        "class_report = classification_report(y_true, y_pred, target_names=label_encoder.classes_, output_dict=True)\n",
        "\n",
        "# Store comprehensive results\n",
        "results = {\n",
        "    'model_name': 'ResNet-Transformer',\n",
        "    'accuracy': float(accuracy),\n",
        "    'precision': float(precision_macro),\n",
        "    'recall': float(recall_macro),\n",
        "    'f1_score': float(f1_macro),\n",
        "    'auc': float(auc),\n",
        "    'confusion_matrix': cm.tolist(),\n",
        "    'classification_report': class_report,\n",
        "    'predictions': y_pred.tolist(),\n",
        "    'true_labels': y_true.tolist(),\n",
        "    'prediction_probabilities': y_pred_proba.tolist(),\n",
        "    'training_time': training_time,\n",
        "    'total_parameters': int(total_params),\n",
        "    'sequence_length': 128,\n",
        "    'classes': label_encoder.classes_.tolist()\n",
        "}\n",
        "\n",
        "# Save results as JSON\n",
        "with open('resnet_transformer_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"=== RESULTS SUMMARY ===\")\n",
        "print(f\"Model: ResNet-Transformer\")\n",
        "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
        "print(f\"Precision: {precision_macro*100:.2f}%\")\n",
        "print(f\"Recall: {recall_macro*100:.2f}%\")\n",
        "print(f\"F1-Score: {f1_macro*100:.2f}%\")\n",
        "print(f\"AUC: {auc*100:.2f}%\")\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"Parameters: {total_params:,}\")\n",
        "\n",
        "print(\"\\n=== FILES SAVED ===\")\n",
        "print(\"- resnet_transformer_model_final.keras\")\n",
        "print(\"- resnet_transformer_results.json\")\n",
        "print(\"- resnet_transformer_classification_report.csv\")\n",
        "print(\"- resnet_transformer_sensor_scaler.pkl\")\n",
        "print(\"- resnet_transformer_sensor_label_encoder.pkl\")\n",
        "print(\"- resnet_transformer_results.png (from visualization cell)\")\n",
        "\n",
        "print(\"\\n=== DOWNLOAD FILES ===\")\n",
        "print(\"Run the next cell to download all results files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download Results Files for Google Colab\n",
        "print(\"Downloading all result files...\")\n",
        "\n",
        "# Create a zip file with all results\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# List of files to include in the download\n",
        "files_to_download = [\n",
        "    'resnet_transformer_model_final.keras',\n",
        "    'resnet_transformer_results.json', \n",
        "    'resnet_transformer_classification_report.csv',\n",
        "    'resnet_transformer_sensor_scaler.pkl',\n",
        "    'resnet_transformer_sensor_label_encoder.pkl'\n",
        "]\n",
        "\n",
        "# Add PNG file if it exists\n",
        "if os.path.exists('resnet_transformer_results.png'):\n",
        "    files_to_download.append('resnet_transformer_results.png')\n",
        "\n",
        "# Create zip file\n",
        "with zipfile.ZipFile('resnet_transformer_results.zip', 'w') as zipf:\n",
        "    for file in files_to_download:\n",
        "        if os.path.exists(file):\n",
        "            zipf.write(file)\n",
        "            print(f\"Added {file} to download\")\n",
        "\n",
        "# Download the zip file\n",
        "files.download('resnet_transformer_results.zip')\n",
        "\n",
        "print(\"\\n=== RESNET-TRANSFORMER MODEL EXECUTION COMPLETE ===\")\n",
        "print(\"All results have been saved and downloaded!\")\n",
        "print(\"You can now use these files for your journal publication.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
